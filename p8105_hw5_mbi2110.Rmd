---
title: "Homework 5"
author: "Mia Isaacs"
date: "2024-11-10"
output: github_document
---

```{r}
library(tidyverse)
library(broom)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(123)
```

# Problem 1 

## write birthday function

```{r}
bday_sim = function(n) {

  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n

  return(duplicate)
  
}

bday_sim(10)
```

## run lots of times

```{r}
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))
```

## make plot showing prob by group size

```{r}
sim_res |> 
  ggplot(aes(x = n, y = prob )) + 
  geom_line()
```

As group size increases, the probability that at least two people in the group share the same birthday also increases. With a group size of 50, there is almost a 100% chance that two people will share the same birthday.

# Problem 2

## set design elements

```{r}
prob2_sim = function(samp_size = 30, true_mean = 0, true_sd = 5, alpha = .05) {
  
  prob2_df =
    tibble(
      x = rnorm(samp_size, mean = true_mean, sd = true_sd)
    )
  
  ttest_result = tidy(t.test(prob2_df[["x"]], mu = 0)) 
  
  out_df =
    tibble(
      estimated_mean = ttest_result |> pull(estimate),
      p_value = ttest_result |> pull(p.value)
    )
  
  return(out_df)
}

prob2_sim(samp_size = 30, true_mean = 1)
```

## generate datasets

```{r}
sim_results =
  expand_grid(
    true_mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    ttest_result = map(true_mu, ~prob2_sim(samp_size = 30, true_mean = .x, true_sd = 5))
  ) |>
  unnest(ttest_result)

sim_results |> 
  group_by(true_mu) |> 
  summarize(
    avg_mean = mean(estimated_mean, na.rm = TRUE),
    p_value = mean(p_value,  na.rm = TRUE),
    .groups = "drop"
  )
```

## calculate power

```{r}
alpha = 0.05 

power_results =
  sim_results |> 
  group_by(true_mu) |> 
  summarize(
    power = mean(p_value < alpha),
    avg_est_mu = mean(estimated_mean),
    avg_est_reject = mean(estimated_mean[p_value < alpha])
  )
```

## plot power vs. true mean

```{r}
power_results |> 
  ggplot(aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power vs. True Mean",
    x = "True Value of Mu",
    y = "Power"
  )
```

As effect size increases, power increases. We see an increase in ability to detect a significant difference as the mean value moves further away from the null value of 0.

## plot estimated mean vs. true mean

```{r}
power_results |> 
  ggplot(aes(x = true_mu)) +
  geom_line(aes(y = avg_est_mu, color = "All Samples")) +
  geom_point(aes(y = avg_est_mu, color = "All Samples")) +
  geom_line(aes(y = avg_est_reject, color = "Rejected Samples Only")) +
  geom_point(aes(y = avg_est_reject, color = "Rejected Samples Only")) +
  labs(
    title = "Average Estimate of Mean vs. True Mean",
    x = "True Value of Mu",
    y = "Average Estimate of Mu"
  )
```

The average estimated mean among tests in which the null was rejected differs from the true value between the values of 0 and 4, where the average estimates are higher than the true values. This is because the power to detect a significant difference is lower when our effect size is lower, therefore a larger difference between the estimated value and the true value is required to reject the null hypothesis at lower effect sizes.

# Problem 3

## read in data

```{r}
homicide_data = read_csv("data/homicide-data.csv")

summary(homicide_data)
```

The homicide_data includes `r nrow(homicide_data)` observations and `r ncol(homicide_data)` variables. Variables include city id, reported date, victim first and last name, race, age, sex, city, state, latitude, longitude, and disposition.

## create city state variable and summarize

```{r}
homicide_summary = 
  homicide_data |> 
  mutate(city_state = str_c(city, state, sep = ", ")) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) |> 
  ungroup()
```

## proportion unsolved in Baltimore

```{r}
baltimore_df =
  homicide_summary |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    test = list(prop.test(unsolved_homicides, total_homicides) |> 
                  tidy())
  ) |> 
  unnest(test) |> 
  select(estimate,conf.low, conf.high)

baltimore_df
```

## run for all cities

```{r}
all_cities_df =
  homicide_summary |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = sum(total_homicides),
    unsolved_homicides = sum(unsolved_homicides),
    .groups = "drop"
  ) |> 
  mutate(
    test_results = map2(unsolved_homicides, total_homicides, ~ tidy(prop.test(.x, .y)))
  ) |> 
  unnest(test_results) |> 
  select(city_state, estimate, conf.low, conf.high)

all_cities_df
```

## plot with estimates for each city

```{r}
all_cities_df |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate, color = city_state)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.25, color = "darkgrey") +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```






